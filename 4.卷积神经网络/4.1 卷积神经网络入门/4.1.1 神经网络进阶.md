# 4.1.1 神经网络进阶

- 多层神经元-神经网络

  以下的神经网络，具有一个隐含层，一个输出层

  其中输入依旧是x1,x2,x3,1（一个数据的3个维度加一个泛化1）

  隐含层有4个神经元，输出层有一个神经元，他们的计算公式和上节课介绍的逻辑斯蒂回归模型一样

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-916ee39f41ff0f5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 神经网络正向计算

  多层的神经网络，需要从低到高，一层一层的计算出最后的结果，当然这些计算是可以并行计算的（通过矩阵乘法）

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-7b886273b54ba923.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 神经网络的训练

  在回顾一下上节课讲的神经网络的训练，是用损失函数对每一个参数求偏导，在用这个偏导乘以一个学习率α，再用这个结果去更新所有的参数，一步一步的迭代，最终得到一个比较好的神经网络

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-a9b2fff4754aab52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 神经网络的训练-反向传播

  在多层神经网络中，我们如何给每一个参数去计算他的偏导数。

  首先先看h21-h的这一层所有的参数的导数是如何计算的，假设这里还是使用的sigmiod激活函数，使用平方差的损失函数

  这里的w3存在于h21,h22,h23和h的连线上，X就是h21,h22,h23的输出和+1

  w2存在于h11,h12,h13和h21,h22,h23的各种连线中，也可以说，存在h21,h22,h23的表达式中

  w1同理

  相当于说w1,w2都是以复合函数的形式参与到损失函数的计算中来，对于这种复合函数的求导使用的是链式法则

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-f20e537b6452b857.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 链式法则

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-d9b13877d911fdb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 神经网络训练优化

  上面这种训练的缺点：

  - 每次都在整个数据集上计算Loss和梯度
    - 计算量大
    - 可能内存承载不住
  - 梯度方向确定的时候，仍然是每次都走一个单位步长
    - 耗时

  优化方案

  - 随机梯度下降

    - 去掉每次使用整个数据集的假设，每次只使用一个样本。这样会有一个问题，就是说一个样本其实并不能反映整个数据集的一个方向，所以会导致他的收敛速度比较慢

      > （收敛速度：模型从开始训练到结束训练，开始非常快，后来比较慢，慢慢达到一个稳定的值，当达到一个比较稳定的值，我们称之为收敛）

  - mini-batch梯度下降

    - 每次使用小部分数据进行训练，这一小部分数据是随机采样出来的，因为是随机的，所以可以代表整个数据集的梯度方向，从而既不会有全部数据集的占内存大，训练慢的确定，又不会有随机梯度下降不能够反映整个数据集梯度下降方向的问题

  mini-batch这样的梯度下降的其他问题

  - 梯度下降存在震荡的问题，一个样本是不能够反映全部数据集的方向的，mini-batch中的数据虽然是随机采样出来的，但是因为随机采样的数据量不够多，所以仍然会导致梯度下降存在一个震荡的问题，这个震荡的问题在单个样本上反映的更明显，对于mini-batch来说，size越大越不明显

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-262eb2a7ee881099.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

  - 局部极值问题

    一个函数不一定是个凸函数（凸函数：只有一个极小值的函数），不一定只有一个最优解，如下右图。

    这样的一个函数会导致，当我到了一个局部最优解的时候，不管我的梯度计算方向在哪，只要我的learning rate不是那么大，都会导致他往回去学，所以running rate是一个比较重要的参数，如果learning rate设置的太小，会导致整个的参数停在局部极值点的位置

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-7f16bbf7a26cd98c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

  - saddle point 

    某一个点的导数为0，如果导数为0，不管α多大，所有参数的更新都是0，导致参数不能得到更新而停在这里，这个不管用mini-batch还是随机梯度下降都会遇到的问题

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-04fd95defceb7b7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

  上面问题的解决方案

  - 动量梯度下降

    vt可以看做是前t-1步的梯度积累值的均值和当前梯度的和，因为每次计算vt的时候，vt-1都乘以了一个系数，这个系数可以控制之前积累的梯度其的作用的大小。

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-e508cade0694764d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    对于梯度方向来说，由于他是有方向的，因此当前梯度和积累值的加法，不光体现在大小上，还体现在方向上。如果momentum step和gradient step这两个向量他的方向差距比较大的时候，他们加在一起的值就会比较小，这样就可以防止震荡；而当他们两个方向差距比较小（或者和在一起）的时候，他们矢量相加的结果抵消的就会比较少，他们的和就会比较大，那么他们加完之后，就会比原来大很多

    这样就可以，在模型刚开始的时候，他的梯度方向是比较稳定的，这个时候由于梯度是有积累的，就可以使得我的梯度模型变的非常大

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-34279c92a4ed8412.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

  - 动量梯度下降的特点

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-6aec4e559ab84001.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    - 开始训练时,积累动量,加速训练
    - 局部极值附近震荡时,梯度为0 ,由于动量,跳出陷阱
    - 梯度改变方向的时候,动量缓解动荡

- 