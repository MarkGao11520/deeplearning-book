# 4.1.1 神经网络进阶

- 多层神经元-神经网络

  以下的神经网络，具有一个隐含层，一个输出层

  其中输入依旧是x1,x2,x3,1（一个数据的3个维度加一个泛化1）

  隐含层有4个神经元，输出层有一个神经元，他们的计算公式和上节课介绍的逻辑斯蒂回归模型一样

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-916ee39f41ff0f5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 神经网络正向计算

  多层的神经网络，需要从低到高，一层一层的计算出最后的结果，当然这些计算是可以并行计算的（通过矩阵乘法）

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-7b886273b54ba923.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 神经网络的训练

  在回顾一下上节课讲的神经网络的训练，是用损失函数对每一个参数求偏导，在用这个偏导乘以一个学习率α，再用这个结果去更新所有的参数，一步一步的迭代，最终得到一个比较好的神经网络

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-a9b2fff4754aab52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 神经网络的训练-反向传播

  在多层神经网络中，我们如何给每一个参数去计算他的偏导数。

  首先先看h21-h的这一层所有的参数的导数是如何计算的，假设这里还是使用的sigmiod激活函数

- 