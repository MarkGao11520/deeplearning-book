# 4.3.2  激活函数到调参技巧

- 更多的优化算法

  - 学习率自适应

    让学习率自己去调整，设置一个初始值，然后让他按照某些规则自己去衰减

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-48b5fbe199463ae6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

  - 对于稀疏数据,使用学习率可自适应方法。

  - SGD通常训练时间更长,最终效果比较好,但需要好的初始化和learning rate

  - 需要训练较深较复杂的网络且需要快速收敛时,推荐使用adam。

  - Adagrad , RMSprop , Adam是比较相近的算法,在相似的情况下表现差不多。

- 激活函数

  - 回顾

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-c95220adfc60c755.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

  - Sigmoid

    - 输入非常大或非常小的时候没有梯度

    - 输出的均值非0

      可能会造成第一层是归一化好的，但是后面都不是归一化好的，这对神经网络的学习是不友好的

    - Exp计算复杂，e的-x次方计算会比较复杂，使得训练比较慢

    - 梯度消失

      df(x)/dx = f(x)(1-f(x)) 由于fx < 1 所以这两项相乘就是一个更小的数，在层次比较深的情况下，会导致底层的参数得不到更新(10的-n次方)

  - Tanh

    - 在比较小或者比较大的时候依旧没有梯度
    - 输出均值是0
    - 计算依旧很复杂

  - ReLU

    - 不饱和（梯度不会过小）
    - 计算量小
    - 收敛速度快
    - 输出均值非0
    - Dead ReLU
      - 一个非常大的梯度流过神经元，不会再对数据有激活现象了

  - Leaky-ReLU

    - 解决了dead relu的问题

  - ELU

    - 均值更解决于0
    - 小于0的时候计算量比较大

  - Maxout

    - ReLU的泛化版本

      当w2和b2都是0的时候就等价于relu

    - 没有dead relu（因为w2和b2通常不等于0）

    - 没有double

  - 使用激活函数的技巧

    - ReLu-小心设置learning rate
    - 不要使用sigmoid
    - 使用Leaky ReLU、maxout，ELU
    - 可以使用tanh，但不要抱太大期望

- 网络初始化

  网络初始化是一个网络调参的重要过程，如果初始化好的话，那么他能够达到一个效果就是即训练的快又能得到一个很好的结果，如果不好的话就会使得网络不容易训练。

  - 全部为0

    - 单层网络可以
    - 多层网络会使得梯度消失（链式法则）

  - 如何分析初始化结果好不好？

    - 查看初始化后各层的激活值分布

      如果分布是在固定的区间内，我们就认为是一个号的初始化结果，如果集中在一部分值上，就认为他是不好的。

      因为参数是各不相同的，所以期望保持一个比较好的梯度更新，由于每一个参数的计算和之前激活值是有很大关系的，所以激活值不一样，就代表最后的梯度可能是不一样的，所以就能使得每个参数能得到一个好的梯度，最后达到一个比较快的学习过程

  - 正态分布初始化

    - 均值为0，方差为0.2 的正态分布初始化-Tanh

      Tanh 高层均值为0，没有梯度，导致这种结果的原因是：方差太小了

      ![image.png](https://upload-images.jianshu.io/upload_images/7220971-b4d1be6f1dad689b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    - 均值为0，方差为1 的正态分布初始化-Tanh

      高峰均值为-1.1，已经饱和

      tanh，s型，输出值在[-1,1]之间，输入在[-6,6]变化的比较快，外面变化的比较小

      ![image.png](https://upload-images.jianshu.io/upload_images/7220971-f6ebde1ac45fabc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    - Xavier-tanh

      通过数学推导，发现了一些公式

      ```python
      W = np.random.randmn(fan_in,fan_out) / np.sqrt(fan_in)
      ```

      使用这样的初始化，可以使得各层的激活值分布是一个均匀的分布

      ![image.png](https://upload-images.jianshu.io/upload_images/7220971-66c011b24fca70e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    - Xavier-relu

      ![image.png](https://upload-images.jianshu.io/upload_images/7220971-fb9d604930e43408.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    - He-Relu

      在Xavier-relu上做了改进

      ```
      W = np.random.randmn(fan_in,fan_out) / np.sqrt(fan_in/2)
      ```



      ![image.png](https://upload-images.jianshu.io/upload_images/7220971-2477427171607c35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

  - 批归一化（BN）

    从上面的分析可以发现，某个激活函数有起特定效果好的参数初始化，如果换了个激活函数导致训练结果不好，不一定是激活函数的问题，也有可能是参数初始化的问题。这是一个多因素决定的结果

    那么有什么方法可以使得每一层的分布都变的比较统一呢—批归一化

    - 每个batch在每一层都做归一化

      但是一个batch不能反应全部的数据

    - 为了确保归一化能够起到作用，另设两个参数来做逆归一化，这两个参数是需要学习的

      γ和β就是这两个参数，分别相当于均值和方差

      这样就能够保证原来提取到的特征仍然是有效的，当归一化没有效果的时候，仍然可以逆回来，使得原来的特征能够保存下来

      ![image.png](https://upload-images.jianshu.io/upload_images/7220971-c3cb9b7f416e9daf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 数据增强

  - 归一化

- 

