# 1.2 神经元多输出

- 神经元-多输出

  处理二分类的问题，用一个神经元可以解决，处理多分类的问题，加神经元就可以了。这时候之前的w向量变成了矩阵

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-b463f7b4905a169a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 多输出的例子

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-e04fc5308420b5db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 二分类逻辑斯蒂回归模型的另一种角度-归一化

  ![image-20180923214716250](/Users/gaowenfeng/Library/Application Support/typora-user-images/image-20180923214716250.png)

- 多分类逻辑斯蒂回归模型的推导

  ![image.png](https://upload-images.jianshu.io/upload_images/7220971-8d0923bc9ffb3a22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 多分类逻辑斯蒂回归模型的例子

  ![image-20180923215036527](/Users/gaowenfeng/Library/Application Support/typora-user-images/image-20180923215036527.png)

- 目标函数（通常也被成为损失函数）

  - 衡量对数据的拟合程度

  - 举个例子

    - (x1,y1) = ([10,3,9,20,....,4],1)  二分类问题，y1 可能等于0或者1，模型算出的概率大于0.5，就认为是1这个类，小于0.5，就认为是0这个类
    - y1<sup>、</sup> = Model(x1) = 0.8 假设模型算出来的概率值为0.8
    - Loss = y1 - y1<sup>、</sup> = 0.2  调整神经网络，就是调整model中的参数，使得y1<sup>、</sup> 更大，Loss更小

  - 再多举个多分类的问题

    - (x1,y1) = ([10,3,9,0....4], 3)
    - y1<sup>、</sup> = Model(x1) = [0.1,0.2,0.25,04,0.05]
    - 3->[0,0,0,1,0]  数值到向量的变换叫做one-hot编码
    - Loss = abs(y1- y1<sup>、</sup>) = [0,0,0,1,0] - y1<sup>、</sup> = [0.1,0.2,0.25,0.6,0.05] = 1.2   整个向量相减得到目标值

  - 平方差损失

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-cfc884ac2029ad29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    先求出每个数据的目标值（实数就直接相减再平方，向量就向量减法再计算向量平方值），然后求和所有目标值，再做平均

  - 交叉熵损失函数

    ![image.png](https://upload-images.jianshu.io/upload_images/7220971-e8265fab7e571eb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

    这是个熵函数，就是用来衡量两个分布直接的差距的，所以他更适合去做多分类的损失函数

  - 调整参数使模型在训练集上的损失函数最小, 这就以为着model预测出来的结果和真实值的差距最小
