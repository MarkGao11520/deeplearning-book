
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>2.4 神经网络的训练 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="2.5 随机梯度下降法.html" />
    
    
    <link rel="prev" href="2.3 梯度下降法.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../1.神经网络入门/readme.html">
            
                <a href="../1.神经网络入门/readme.html">
            
                    
                    1.神经网络入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="readme.html">
            
                <a href="readme.html">
            
                    
                    2.前馈神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="2.1 网络结构.html">
            
                <a href="2.1 网络结构.html">
            
                    
                    2.1 网络结构
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="2.2 网络结构_代码部分.html">
            
                <a href="2.2 网络结构_代码部分.html">
            
                    
                    2.2 网络结构_代码部分
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="2.3 梯度下降法.html">
            
                <a href="2.3 梯度下降法.html">
            
                    
                    2.3 梯度下降法
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.4" data-path="2.4 神经网络的训练.html">
            
                <a href="2.4 神经网络的训练.html">
            
                    
                    2.4 神经网络的训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.5" data-path="2.5 随机梯度下降法.html">
            
                <a href="2.5 随机梯度下降法.html">
            
                    
                    2.5 随机梯度下降法
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../3.提高神经网络学习效率/readme.html">
            
                <a href="../3.提高神经网络学习效率/readme.html">
            
                    
                    3.提高神经网络学习效率
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../4.卷积神经网络/readme.html">
            
                <a href="../4.卷积神经网络/readme.html">
            
                    
                    4.卷积神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../4.卷积神经网络/4.1 卷积神经网络入门/readme.html">
            
                <a href="../4.卷积神经网络/4.1 卷积神经网络入门/readme.html">
            
                    
                    4.1 卷积神经网络入门
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1.1" data-path="../4.卷积神经网络/4.1 卷积神经网络入门/4.1.1 神经网络进阶.html">
            
                <a href="../4.卷积神经网络/4.1 卷积神经网络入门/4.1.1 神经网络进阶.html">
            
                    
                    4.1 卷积神经网络入门
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../4.卷积神经网络/4.2 卷积神经网络进阶/readme.html">
            
                <a href="../4.卷积神经网络/4.2 卷积神经网络进阶/readme.html">
            
                    
                    4.2 卷积神经网络进阶
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="../4.卷积神经网络/4.3 卷积神经网络调参/readme.html">
            
                <a href="../4.卷积神经网络/4.3 卷积神经网络调参/readme.html">
            
                    
                    4.3 卷积神经网络调参
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="../4.卷积神经网络/4.4 图像风格转换/readme.html">
            
                <a href="../4.卷积神经网络/4.4 图像风格转换/readme.html">
            
                    
                    4.4 图像风格转换
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../5.循环神经网络/readme.html">
            
                <a href="../5.循环神经网络/readme.html">
            
                    
                    5.循环神经网络
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="../5.循环神经网络/5.1 循环神经网络入门到进阶/readme.html">
            
                <a href="../5.循环神经网络/5.1 循环神经网络入门到进阶/readme.html">
            
                    
                    5.1 循环神经网络入门到进阶
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="../5.循环神经网络/5.2 图像生成文本/readme.html">
            
                <a href="../5.循环神经网络/5.2 图像生成文本/readme.html">
            
                    
                    5.2 图像生成文本
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../6.对抗神经网络/readme.html">
            
                <a href="../6.对抗神经网络/readme.html">
            
                    
                    6.对抗神经网络
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >2.4 神经网络的训练</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="24-&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;">2.4 &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;</h1>
<h3 id="1-&#x524D;&#x5411;&#x4F20;&#x64AD;">1. &#x524D;&#x5411;&#x4F20;&#x64AD;</h3>
<ul>
<li>&#x521D;&#x59CB;&#x5316;w,b</li>
<li>Loss(w,b)</li>
<li>&#x632A;&#x52A8;w,b &#x9010;&#x6B65;&#x53D8;&#x5316;, &#x76F4;&#x5230;Loss&#x8DB3;&#x591F;&#x5C0F;&#x3002;</li>
</ul>
<p>&#x5047;&#x8BBE;&#x6211;&#x4EEC;&#x6709;10&#x4E2A;&#x6837;&#x672C;&#xFF0C;&#x6BCF;&#x4E2A;&#x6837;&#x672C;&#x7684;&#x8F93;&#x5165;&#x90FD;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x6570;x&#xFF0C;&#x8F93;&#x51FA;&#x662F;&#x4E00;&#x4E2A;&#x5B9E;&#x6570;y&#x3002;</p>
<p>&#x6839;&#x636E;&#x7F51;&#x7EDC;&#x4E2D;&#x4E24;&#x4E2A;&#x795E;&#x7ECF;&#x5143;&#x7684;&#x8868;&#x8FBE;&#x5F0F;&#x7684;&#x63CF;&#x8FF0;&#xFF0C;&#x9996;&#x5148;&#x53EF;&#x4EE5;&#x5E26;&#x5165;x1&#xFF0C;&#x8868;&#x8FBE;&#x5F0F;&#x7684;&#x6620;&#x5C04;&#x5173;&#x7CFB;&#x5C31;&#x53D8;&#x6210;&#x4E86;&#x4E0B;&#x56FE;&#x4E2D;&#x7684;&#x7B2C;&#x4E09;&#x884C;&#x548C;&#x7B2C;&#x56DB;&#x884C;&#x3002;</p>
<p>&#x8FD9;&#x65F6;x1&#xFF0C;y1&#x6240;&#x5E26;&#x6765;&#x7684;&#x8BEF;&#x5DEE;&#x503C;&#x4E5F;&#x53EF;&#x4EE5;&#x5B9A;&#x4E49;&#x4E86;&#xFF0C;&#x5373;Loss<sub>1</sub>,&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x6837;&#x672C;&#x7684;Loss&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x6B8B;&#x5DEE;&#x5E73;&#x65B9;&#x3002; &#x7136;&#x540E;&#x7D2F;&#x52A0;&#x6C42;&#x548C;&#x5F97;&#x5230;&#x5168;&#x90E8;&#x7684;&#x8BEF;&#x5DEE;&#x548C;Loss</p>
<p>&#x5982;&#x679C;&#x9884;&#x6D4B;&#x51FA;&#x6765;&#x7684;&#x7ED3;&#x679C;yo1&#x548C;y&#x5F88;&#x63A5;&#x8FD1;&#xFF0C;loss1&#x5C31;&#x4F1A;&#x6BD4;&#x8F83;&#x5C0F;&#x3002;</p>
<p>&#x56E0;&#x4E3A;&#x6211;&#x4EEC;&#x6709;10&#x5BF9;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#xFF0C;&#x5F97;&#x5230;&#x5341;&#x4E2A;loss&#xFF0C;Loss&#x7D2F;&#x52A0;&#x3002;</p>
<p>&#x8FD9;&#x5C31;&#x662F;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x4E2D;&#x7684;&#x524D;&#x5411;&#x4F20;&#x64AD;: &#x8F93;&#x5165;&#x6570;&#x636E;&#x901A;&#x8FC7;&#x7F51;&#x7EDC;&#x4E00;&#x5C42;&#x4E00;&#x5C42;&#x7684;&#x4F5C;&#x7528;&#x4E00;&#x76F4;&#x5411;&#x524D;&#x4F20;&#x64AD;&#x3002;</p>
<p>&#x8BA1;&#x7B97;&#x673A;&#x81EA;&#x5DF1;&#x5B66;&#x4E60;&#x51FA;&#x56DB;&#x4E2A;&#x53C2;&#x6570;&#xFF1A;</p>
<p><img src="../Users/gaowenfeng/GitBook/Library/Import/deeplearning-book/image-20180918233052167.png" alt="image-20180918233052167"></p>
<h3 id="2-&#x524D;&#x5411;&#x4F20;&#x64AD;&#x7684;&#x4EE3;&#x7801;&#x90E8;&#x5206;">2. &#x524D;&#x5411;&#x4F20;&#x64AD;&#x7684;&#x4EE3;&#x7801;&#x90E8;&#x5206;</h3>
<pre><code>   ```python
</code></pre><h1 id="codingutf-8">coding=utf-8</h1>
<p>import random
import numpy as np</p>
<p>class Network(object):
    def <strong>init</strong>(self, sizes):</p>
<pre><code>    # &#x7F51;&#x7EDC;&#x5C42;&#x6570;
    self.num_layers = len(sizes)
    # &#x7F51;&#x7EDC;&#x6BCF;&#x5C42;&#x795E;&#x7ECF;&#x5143;&#x4E2A;&#x6570;
    self.sizes = sizes
    # &#x521D;&#x59CB;&#x5316;&#x6BCF;&#x5C42;&#x7684;&#x504F;&#x7F6E;
    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
    # &#x521D;&#x59CB;&#x5316;&#x6BCF;&#x5C42;&#x7684;&#x6743;&#x91CD;
    self.weights = [np.random.randn(y, x)
                    for x, y in zip(sizes[:-1], sizes[1:])]

# &#x68AF;&#x5EA6;&#x4E0B;&#x964D;
def GD(self, training_data, epochs):
    # &#x5F00;&#x59CB;&#x8BAD;&#x7EC3; &#x5FAA;&#x73AF;&#x6BCF;&#x4E00;&#x4E2A;epochs
    for j in xrange(epochs):
        # &#x6D17;&#x724C; &#x6253;&#x4E71;&#x8BAD;&#x7EC3;&#x6570;&#x636E;
        random.shuffle(training_data)

        # &#x8BAD;&#x7EC3;&#x6BCF;&#x4E00;&#x4E2A;&#x6570;&#x636E;
        for x, y in training_data:
            self.update(x, y)

        print &quot;Epoch {0} complete&quot;.format(j)

# &#x524D;&#x5411;&#x4F20;&#x64AD;
def update(self, x, y):
    activation = x

    # &#x4FDD;&#x5B58;&#x6BCF;&#x4E00;&#x5C42;&#x7684;&#x6FC0;&#x52B1;&#x503C;a=sigmoid(z)
    activations = [x]

    # &#x4FDD;&#x5B58;&#x6BCF;&#x4E00;&#x5C42;&#x7684;z=wx+b
    zs = []
    # &#x524D;&#x5411;&#x4F20;&#x64AD;
    for b, w in zip(self.biases, self.weights):
        # &#x8BA1;&#x7B97;&#x6BCF;&#x5C42;&#x7684;z
        z = np.dot(w, activation) + b

        # &#x4FDD;&#x5B58;&#x6BCF;&#x5C42;&#x7684;z
        zs.append(z)

        # &#x8BA1;&#x7B97;&#x6BCF;&#x5C42;&#x7684;a
        activation = sigmoid(z)

        # &#x4FDD;&#x5B58;&#x6BCF;&#x4E00;&#x5C42;&#x7684;a
        activations.append(activation)
</code></pre><p>def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))</p>
<pre><code>   ```
</code></pre><p>&#x8BA1;&#x7B97;&#x6BCF;&#x4E00;&#x5C42;&#x7684;z, &#x518D;&#x8BA1;&#x7B97;&#x6BCF;&#x4E00;&#x5C42;&#x7684;a&#x3002;&#x63A5;&#x7740;&#x4FDD;&#x5B58;&#x6BCF;&#x4E00;&#x5C42;&#x7684;z&#x548C;a&#xFF0C;&#x4E3A;&#x540E;&#x9762;&#x7684;&#x53CD;&#x5411;1&#x4F20;&#x64AD;&#x505A;&#x51C6;&#x5907;&#x3002;</p>
<h3 id="3-&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x66F4;&#x65B0;&#x53C2;&#x6570;">3. &#x53CD;&#x5411;&#x4F20;&#x64AD;&#x66F4;&#x65B0;&#x53C2;&#x6570;</h3>
<p>&#x53EF;&#x4EE5;&#x5F00;&#x59CB;&#x632A;&#x52A8;Wh Bh Wo Bo&#x56DB;&#x4E2A;&#x5F85;&#x5B9A;&#x7CFB;&#x6570;&#xFF0C;&#x6765;&#x9010;&#x6B65;&#x51CF;&#x5C0F;Loss&#x503C;&#x3002;
&#x8FD9;&#x4E2A;&#x5728;&#x4E4B;&#x524D;&#x7684;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x7684;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B;&#x4E2D;&#x6211;&#x4EEC;&#x5DF2;&#x7ECF;&#x8BB2;&#x8FC7;&#x4E86;&#x3002;</p>
<p>&#x4E5F;&#x5C31;&#x662F;&#x9700;&#x8981;&#x56DB;&#x4E2A;&#x66F4;&#x65B0;&#x516C;&#x5F0F;&#x6765;&#x5E2E;&#x6211;&#x4EEC;&#x66F4;&#x65B0;&#x3002;</p>
<p><img src="../Users/gaowenfeng/Library/Application Support/typora-user-images/image-20180920220751139.png" alt="image-20180920220751139"></p>
<p>&#x548C;&#x4E4B;&#x524D;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x4E2D;&#x7684;&#x66F4;&#x65B0;&#x516C;&#x5F0F;&#x5927;&#x540C;&#x5C0F;&#x5F02;&#x3002;&#x53EA;&#x4E0D;&#x8FC7;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x5F53;&#x4E2D;&#x662F;&#x4E00;&#x4E2A;&#x4E09;&#x7EF4;&#x7A7A;&#x95F4;&#x3002;
 &#x6211;&#x4EEC;&#x53BB;&#x627E;&#x7897;&#x5E95;&#x7684;&#x8FC7;&#x7A0B;&#x3002;&#x8FD9;&#x91CC;&#x53D8;&#x6210;&#x4E86;&#x4E00;&#x4E2A;&#x4E94;&#x7EF4;&#x7A7A;&#x95F4;&#x6211;&#x4EEC;&#x53BB;&#x627E;&#x7897;&#x5E95;&#x7684;&#x8FC7;&#x7A0B;&#x3002;</p>
<p>&#x5728;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x4E2D;&#x53BB;&#x66F4;&#x65B0;&#x7F51;&#x7EDC;&#x4E2D;&#x7684;&#x5F85;&#x5B9A;&#x7CFB;&#x6570;&#x7684;&#x8FC7;&#x7A0B;&#xFF0C;&#x53EB;&#x505A;&#x53CD;&#x5411;&#x66F4;&#x65B0;&#x3002;&#x56E0;&#x4E3A;&#x6211;&#x4EEC;&#x662F;&#x4ECE;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x5F00;&#x59CB;&#xFF0C;&#x5012;&#x6570;&#x7B2C;&#x4E8C;&#x5C42;&#xFF0C;&#x5012;&#x6570;&#x7B2C;&#x4E09;&#x5C42;&#x3002;&#x8FD9;&#x6837;&#x4E00;&#x5C42;&#x4E00;&#x5C42;&#x7684;&#x53CD;&#x5411;&#x66F4;&#x65B0;w&#x548C;b</p>
<p>&#x5982;&#x4F55;&#x6C42;&#x8FD9;&#x56DB;&#x4E2A;&#x516C;&#x5F0F;&#x4E2D;&#x7684;&#x504F;loss &#x504F;wh &#x504F;loss &#x504F;bh &#x504F;loss &#x504F;wo &#x504F;loss &#x504F;bo&#xFF0C;&#x53EF;&#x4EE5;&#x501F;&#x52A9;&#x94FE;&#x5F0F;&#x6C42;&#x5BFC;&#x6CD5;&#x5219;</p>
<p><img src="../Users/gaowenfeng/Library/Application Support/typora-user-images/image-20180920220854201.png" alt="image-20180920220854201"></p>
<p>yo&#x662F;&#x7F51;&#x7EDC;&#x9884;&#x6D4B;&#x51FA;&#x6765;&#x7684;&#x7ED3;&#x679C;&#x662F;&#x4E00;&#x4E2A;&#x5DF2;&#x77E5;&#x91CF;, yi&#x662F;&#x6211;&#x4EEC;&#x7684;&#x6807;&#x7B7E;&#xFF0C;&#x5DF2;&#x77E5;&#x503C;&#x3002;</p>
<p>Zo&#x4E5F;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x524D;&#x5411;&#x66F4;&#x65B0;&#xFF0C;WoYh+Bo&#x5F97;&#x5230;&#x3002;&#x4E5F;&#x662F;&#x4E00;&#x4E2A;&#x5DF2;&#x77E5;&#x503C;&#x3002;</p>
<p>&#x53CD;&#x5411;&#x66F4;&#x65B0;&#xFF0C;&#x66F4;&#x65B0;&#x7F51;&#x7EDC;&#x4E2D;&#x7684;w&#xFF0C;b&#xFF0C;&#x9700;&#x8981;&#x4E0B;&#x9762;&#x51E0;&#x4E2A;&#x516C;&#x5F0F;&#x6765;&#x914D;&#x5408;</p>
<p><img src="../Users/gaowenfeng/Library/Application Support/typora-user-images/image-20180920221111687.png" alt="image-20180920221111687"></p>
<p>&#x3B4;<sub>l</sub>&#x4EE3;&#x8868;&#x7B2C;l&#x5C42;&#x7684;&#x3B4;&#x3002;3&#xFF0C;4&#x5F0F;&#x5206;&#x522B;&#x662F;&#x7528;&#x6765;&#x6C42;&#x89E3;&#x6BCF;&#x4E00;&#x5C42;&#x7684;&#x504F;loss&#x504F;b&#x548C;&#x504F;loss&#x504F;w&#x3002;1&#xFF0C;2&#x5F0F;&#x5206;&#x522B;&#x4EE3;&#x8868;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x3B4;&#x548C;&#x975E;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x3B4;&#x7684;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#x3002;</p>
<ul>
<li>&#x7B2C;&#x4E09;&#x5F0F;&#xFF0C;&#x6C42;&#x89E3;&#x504F;loss&#x504F;b&#xFF0C;&#x5982;&#x679C;&#x662F;&#x6700;&#x540E;&#x4E00;&#x5C42;&#xFF0C;&#x5219;&#x76F4;&#x63A5;&#x5C31;&#x662F;1&#x5F0F;&#xFF08;&#x8FD9;&#x4E2A;&#x662F;&#x6211;&#x4EEC;&#x4E0A;&#x9762;&#x63A8;&#x5BFC;&#x7684;&#xFF09;&#xFF1B;&#x5982;&#x679C;&#x662F;&#x975E;&#x6700;&#x540E;&#x4E00;&#x5C42;&#xFF0C;&#x5219;&#x4F7F;&#x7528;2&#x5F0F;</li>
<li>&#x7B2C;&#x56DB;&#x5F0F;&#xFF0C;&#x6C42;&#x89E3;&#x504F;loss&#x504F;w&#xFF0C;&#x5982;&#x679C;&#x662F;&#x6700;&#x540E;&#x4E00;&#x5C42;&#xFF0C;&#x5219;&#x7528;1&#x5F0F;&#x4E58;&#x4EE5;&#x5012;&#x6570;&#x7B2C;&#x4E8C;&#x5C42;&#x7684;y&#x771F;&#x503C;&#xFF1B;&#x5982;&#x679C;&#x662F;&#x975E;&#x6700;&#x540E;&#x4E00;&#x5C42;&#xFF0C;&#x5219;&#x4F7F;&#x7528;2&#x5F0F;&#x4E58;&#x4EE5;&#x5F53;&#x524D;&#x6C42;&#x89E3;&#x5C42;&#x7684;&#x4E0A;&#x4E00;&#x5C42;&#x7684;y&#x771F;&#x503C;</li>
</ul>
<p>&#x5229;&#x7528;&#x56DB;&#x4E2A;&#x53CD;&#x5411;&#x66F4;&#x65B0;&#x516C;&#x5F0F;&#xFF0C;&#x4E00;&#x6B65;&#x4E00;&#x6B65;&#x7684;&#x6162;&#x6162;&#x6539;&#x53D8;w&#x548C;b&#x3002;&#x76F4;&#x5230;loss&#x6700;&#x5C0F;&#x3002;</p>
<p>&#x7F51;&#x7EDC;&#x4E2D;&#x5B9A;&#x4E49;&#x7684;loss&#x51FD;&#x6570;&#x662F;&#x4E00;&#x4E2A;&#x4E8C;&#x6B21;&#x635F;&#x5931;&#x51FD;&#x6570;&#xFF0C;&#x5982;&#x679C;&#x4F60;&#x4F7F;&#x7528;&#x7684;&#x4E0D;&#x662F;&#x4E8C;&#x6B21;&#x635F;&#x5931;&#x51FD;&#x6570;&#xFF0C;&#x90A3;&#x4E48;&#x6700;&#x540E;&#x63A8;&#x51FA;&#x6765;&#x7684;&#x7ED3;&#x679C;&#x5C31;&#x4E0D;&#x662F;&#x8FD9;&#x56DB;&#x4E2A;&#x66F4;&#x65B0;&#x65B9;&#x7A0B;&#x3002;&#x9700;&#x8981;&#x81EA;&#x5DF1;&#x6765;&#x6839;&#x636E;&#x5B9E;&#x9645;&#x60C5;&#x51B5;&#x8FDB;&#x884C;&#x63A8;&#x5BFC;&#x3002;</p>
<p>&#x5982;&#x679C;&#x4F60;&#x4F7F;&#x7528;&#x7684;&#x662F;&#x6846;&#x67B6;&#x7684;&#x8BDD;&#xFF0C;&#x8FD9;&#x4E9B;&#x63A8;&#x5BFC;&#x7684;&#x6B65;&#x9AA4;&#x4E0D;&#x9700;&#x8981;&#x4F60;&#x81EA;&#x5DF1;&#x53BB;&#x505A;&#xFF0C;&#x4F60;&#x53EA;&#x9700;&#x8981;&#x8BBE;&#x7F6E;&#x53C2;&#x6570;&#x5C31;&#x53EF;&#x4EE5;&#x4E86;&#x3002;</p>
<h3 id="4&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x66F4;&#x65B0;&#x4EE3;&#x7801;&#x5B9E;&#x73B0;">4.&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x66F4;&#x65B0;&#x4EE3;&#x7801;&#x5B9E;&#x73B0;</h3>
<p>&#x5728;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;GD&#x65B9;&#x6CD5;&#x4E2D;&#x52A0;&#x5165;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7684;&#x53C2;&#x6570;&#x4FDD;&#x5B58;</p>
<pre><code>            # &#x53CD;&#x5411;: &#x4FDD;&#x5B58;&#x6BCF;&#x5C42;&#x504F;&#x5BFC;
            # &#x53CD;&#x5411;: &#x53D6;&#x5230;&#x6BCF;&#x4E00;&#x5C42;&#x7684;&#x504F;&#x7F6E;&#x503C;&#xFF0C;&#x53D6;&#x5230;&#x5B83;&#x7684;&#x5F62;&#x72B6;&#xFF0C;&#x4EE5;&#x8FD9;&#x4E2A;&#x5F62;&#x72B6;&#x521B;&#x5EFA;&#x96F6;&#x77E9;&#x9635;
            nabla_b = [np.zeros(b.shape) for b in self.biases]
            nabla_w = [np.zeros(w.shape) for w in self.weights]
</code></pre><p>update&#x65B9;&#x6CD5;&#x4E2D;&#x52A0;&#x5165;&#x504F;&#x5BFC;&#x7684;&#x4FDD;&#x5B58;</p>
<pre><code>    # &#x524D;&#x5411;&#x4F20;&#x64AD;
    def update(self, x, y):
        # &#x4FDD;&#x5B58;&#x6BCF;&#x5C42;&#x504F;&#x5012;
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
</code></pre><p>&#x5C06;z&#x548C;activation&#x4FDD;&#x5B58;&#x4E86;&#x4E4B;&#x540E;&#x5C31;&#x53EF;&#x4EE5;&#x8FDB;&#x884C;&#x53CD;&#x5411;&#x66F4;&#x65B0;&#x4E86;&#x3002;</p>
<p>&#x7F51;&#x7EDC;&#x9884;&#x6D4B;&#x7684;&#x503C;&#x51CF;&#x53BB;&#x5B83;&#x7684;&#x771F;&#x5B9E;&#x503C;&#x4E4B;&#x540E;&#xFF0C;&#x70B9;&#x4E58;:</p>
<p><img src="http://myphoto.mtianyan.cn/blog/180331/aKmAhGF51H.png?imageslim" alt="mark"></p>
<p>Zo&#x4E5F;&#x5C31;&#x662F;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;wx+b&#x7684;&#x503C;&#x3002;&#x4EE3;&#x7801;&#x4E2D;&#x6211;&#x4EEC;&#x5DF2;&#x7ECF;&#x628A;Zo&#x8BA1;&#x7B97;&#x51FA;&#x6765;&#x4FDD;&#x5B58;&#x5230;&#x4E86;zs&#x91CC;&#x9762;&#x3002;
 &#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8BEF;&#x5DEE;delta&#x5C31;&#x7B49;&#x4E8E;&#x6211;&#x4EEC;&#x81EA;&#x5B9A;&#x4E49;&#x7684;cost_derivative&#x51FD;&#x6570;&#x8BA1;&#x7B97;Yo-Yi,&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x9884;&#x6D4B;&#x503C;&#x5C31;&#x662F;activations[-1],&#x53D6;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x8BA1;&#x7B97;&#x51FA;&#x6765;&#x7684;&#x9884;&#x6D4B;&#x503C;yo&#xFF0C;&#x63A5;&#x7740;&#x4E58;&#x4EE5;sigmoid_prime&#x51FD;&#x6570;&#x6765;&#x8BA1;&#x7B97;&#x6700;&#x540E;&#x4E00;&#x5C42;sigmoid&#x7684;&#x504F;&#x5BFC;&#x3002;</p>
<pre><code>        # &#x53CD;&#x5411;&#x66F4;&#x65B0;&#x4E86;: &#x4ECE;&#x5012;&#x6570;&#x7B2C;&#x4E00;&#x5C42;&#x5F00;&#x59CB;
        # &#x8BA1;&#x7B97;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8BEF;&#x5DEE;
        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
</code></pre><p>&#x9884;&#x6D4B;&#x503C;-&#x771F;&#x5B9E;&#x503C;</p>
<pre><code>    def cost_derivative(self, output_activation, y):
        return (output_activation - y)
</code></pre><p>sigmoid(Zo)&#x7684;&#x504F;&#x5BFC;: &#x5BFC;&#x6570;&#x7684;&#x4E00;&#x4E2A;&#x7279;&#x6027;</p>
<pre><code>def sigmoid_prime(z):
    return sigmoid(z) * (1 - sigmoid(z))
        # &#x6700;&#x540E;&#x4E00;&#x5C42;&#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E;&#x7684;&#x5012;&#x6570;
        # &#x504F;loos/&#x504F;b = delta
        # &#x504F;loss/&#x504F;w = &#x5012;&#x6570;&#x7B2C;&#x4E8C;&#x5C42;y &#x4E58;&#x4EE5; delta
        nabla_b[-1] = delta
        # transpose&#x8F6C;&#x7F6E;&#x64CD;&#x4F5C;
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
</code></pre><p>&#x53EF;&#x4EE5;&#x6C42;&#x5012;&#x6570;&#x7B2C;&#x4E8C;&#x5C42;&#x76F4;&#x5230;&#x7B2C;&#x4E00;&#x5C42;&#x7684;&#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E;&#x7684;&#x5BFC;&#x6570;&#x3002;</p>
<p>&#x4ECE;&#x5BFC;&#x6570;&#x7B2C;&#x4E8C;&#x5C42;&#x5F00;&#x59CB;&#xFF0C;&#x6240;&#x4EE5;(2, self.num_layers) zs[-l]</p>
<p>&#x5148;&#x6B63;&#x5411;&#x4F20;&#x64AD;&#x540E;&#x53CD;&#x5411;&#x66F4;&#x65B0;&#x3002;&#x6C42;&#x5F97;&#x4E86;&#x6BCF;&#x4E00;&#x5C42;&#x7684;&#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E;&#x7684;&#x5BFC;&#x6570;&#x3002;</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x5012;&#x6570;&#x7B2C;&#x4E8C;&#x5C42;&#x4E00;&#x76F4;&#x5230;&#x7B2C;&#x4E00;&#x5C42; &#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E;&#x7684;&#x5012;&#x6570;</span>
        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>, self.num_layers):
            z = zs[-l]

            sp = sigmoid_prime(z)

            <span class="hljs-comment"># &#x5F53;&#x524D;&#x5C42;&#x7684;&#x8BEF;&#x5DEE;</span>
            delta = np.dot(self.weights[-l+<span class="hljs-number">1</span>].transpose(), delta) * sp

            <span class="hljs-comment"># &#x5F53;&#x524D;&#x5C42;&#x504F;&#x7F6E;&#x548C;&#x6743;&#x91CD;&#x7684;&#x5012;&#x6570;</span>
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l - <span class="hljs-number">1</span>].transpose())
</code></pre>
<p>&#x5728;&#x8BAD;&#x7EC3;&#x4E2D;&#xFF0C;&#x4FDD;&#x5B58;update&#x8FD4;&#x56DE;&#x7684;&#x7ED3;&#x679C;</p>
<pre><code>  # &#x8BAD;&#x7EC3;&#x6BCF;&#x4E00;&#x4E2A;&#x6570;&#x636E;
            for x, y in training_data:
                delta_nable_b, delta_nabla_w = self.update(x, y)

                # &#x4FDD;&#x5B58;&#x4E00;&#x6B21;&#x8BAD;&#x7EC3;&#x7F51;&#x7EDC;&#x4E2D;&#x6BCF;&#x5C42;&#x7684;&#x504F;&#x5012;
                nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nable_b)]
                nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
</code></pre><p>&#x56E0;&#x4E3A;nb&#x662F;&#x96F6;&#x77E9;&#x9635;&#xFF0C;&#x52A0;&#x4E0A;dnb&#x5F97;&#x51FA;&#x6765;&#x7684;&#x7ED3;&#x679C;&#x5C31;&#x662F;dnb&#xFF08;&#x7B2C;&#x4E00;&#x6B21;&#x5FAA;&#x73AF;&#xFF09;</p>
<p>&#x8FD9;&#x91CC;&#x7684;update&#x8FD4;&#x56DE;&#x7684;&#x7ED3;&#x679C;&#x662F;&#x6BCF;&#x8BAD;&#x7EC3;&#x4E00;&#x4E2A;&#x6570;&#x636E;&#x5F97;&#x5230;&#x7684;loss&#x51FD;&#x6570;&#x7684;&#x504F;&#x5BFC;&#xFF0C;&#x603B;loss&#x662F;&#x6BCF;&#x4E2A;&#x6570;&#x636E;&#x7684;loss&#x6C42;&#x548C;</p>
<p><img src="http://myphoto.mtianyan.cn/blog/180329/L92AfLiiCl.png?imageslim" alt="img"></p>
<p>&#x4E4B;&#x540E;&#x5C31;&#x53EF;&#x4EE5;&#x8FDB;&#x884C;&#x771F;&#x6B63;&#x7684;&#x53CD;&#x5411;&#x66F4;&#x65B0;&#x4EE5;&#x53CA;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x6CD5;&#x4E86;&#x3002;</p>
<pre><code> # eta&#x5B66;&#x4E60;&#x7387;
 # &#x66F4;&#x65B0;&#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E; Wn+1 = wn - eta * nw
            self.weights = [w - (eta) * nw
                            for w, nw in zip(self.weights, nabla_w)]
            self.biases = [b - (eta) * nb
                           for b, nb in zip(self.biases, nabla_b)]
</code></pre><p>&#x5FAA;&#x73AF;&#x904D;&#x5386;&#x6BCF;&#x4E00;&#x5C42;&#x7684;&#x6743;&#x91CD;&#x548C;&#x6743;&#x91CD;&#x7684;&#x5BFC;&#x6570;&#x3002;<code>Wn+1 = wn - eta * nw</code>    </p>
<h3 id="5&#x5B8C;&#x6574;&#x7684;&#x66F4;&#x65B0;&#x4EE3;&#x7801;&#x5982;&#x4E0B;">5.&#x5B8C;&#x6574;&#x7684;&#x66F4;&#x65B0;&#x4EE3;&#x7801;&#x5982;&#x4E0B;</h3>
<pre><code class="lang-python"><span class="hljs-comment"># coding=utf-8</span>
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, sizes)</span>:</span>
        <span class="hljs-comment"># &#x7F51;&#x7EDC;&#x5C42;&#x6570;</span>
        self.num_layers = len(sizes)
        <span class="hljs-comment"># &#x7F51;&#x7EDC;&#x6BCF;&#x5C42;&#x795E;&#x7ECF;&#x5143;&#x4E2A;&#x6570;</span>
        self.sizes = sizes
        <span class="hljs-comment"># &#x521D;&#x59CB;&#x5316;&#x6BCF;&#x5C42;&#x7684;&#x504F;&#x7F6E;</span>
        self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> sizes[<span class="hljs-number">1</span>:]]
        <span class="hljs-comment"># &#x521D;&#x59CB;&#x5316;&#x6BCF;&#x5C42;&#x7684;&#x6743;&#x91CD;</span>
        self.weights = [np.random.randn(y, x)
                        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(sizes[:<span class="hljs-number">-1</span>], sizes[<span class="hljs-number">1</span>:])]

    <span class="hljs-comment"># &#x68AF;&#x5EA6;&#x4E0B;&#x964D;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GD</span><span class="hljs-params">(self, training_data, epochs, eta)</span>:</span>
        <span class="hljs-comment"># &#x5F00;&#x59CB;&#x8BAD;&#x7EC3; &#x5FAA;&#x73AF;&#x6BCF;&#x4E00;&#x4E2A;epochs</span>
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(epochs):
            <span class="hljs-comment"># &#x6D17;&#x724C; &#x6253;&#x4E71;&#x8BAD;&#x7EC3;&#x6570;&#x636E;</span>
            random.shuffle(training_data)

            <span class="hljs-comment"># &#x4FDD;&#x5B58;&#x6BCF;&#x5C42;&#x504F;&#x5012;</span>
            nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]
            nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]

            <span class="hljs-comment"># &#x8BAD;&#x7EC3;&#x6BCF;&#x4E00;&#x4E2A;&#x6570;&#x636E;</span>
            <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> training_data:
                delta_nable_b, delta_nabla_w = self.update(x, y)

                <span class="hljs-comment"># &#x4FDD;&#x5B58;&#x4E00;&#x6B21;&#x8BAD;&#x7EC3;&#x7F51;&#x7EDC;&#x4E2D;&#x6BCF;&#x5C42;&#x7684;&#x504F;&#x5012;</span>
                nabla_b = [nb + dnb <span class="hljs-keyword">for</span> nb, dnb <span class="hljs-keyword">in</span> zip(nabla_b, delta_nable_b)]
                nabla_w = [nw + dnw <span class="hljs-keyword">for</span> nw, dnw <span class="hljs-keyword">in</span> zip(nabla_w, delta_nabla_w)]

            <span class="hljs-comment"># &#x66F4;&#x65B0;&#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E; Wn+1 = wn - eta * nw</span>
            self.weights = [w - (eta) * nw
                            <span class="hljs-keyword">for</span> w, nw <span class="hljs-keyword">in</span> zip(self.weights, nabla_w)]
            self.biases = [b - (eta) * nb
                           <span class="hljs-keyword">for</span> b, nb <span class="hljs-keyword">in</span> zip(self.biases, nabla_b)]

            <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Epoch {0} complete&quot;</span>.format(j)

    <span class="hljs-comment"># &#x524D;&#x5411;&#x4F20;&#x64AD;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(self, x, y)</span>:</span>
        <span class="hljs-comment"># &#x4FDD;&#x5B58;&#x6BCF;&#x5C42;&#x504F;&#x5012;</span>
        nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]
        nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]

        activation = x

        <span class="hljs-comment"># &#x4FDD;&#x5B58;&#x6BCF;&#x4E00;&#x5C42;&#x7684;&#x6FC0;&#x52B1;&#x503C;a=sigmoid(z)</span>
        activations = [x]

        <span class="hljs-comment"># &#x4FDD;&#x5B58;&#x6BCF;&#x4E00;&#x5C42;&#x7684;z=wx+b</span>
        zs = []
        <span class="hljs-comment"># &#x524D;&#x5411;&#x4F20;&#x64AD;</span>
        <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):
            <span class="hljs-comment"># &#x8BA1;&#x7B97;&#x6BCF;&#x5C42;&#x7684;z</span>
            z = np.dot(w, activation) + b

            <span class="hljs-comment"># &#x4FDD;&#x5B58;&#x6BCF;&#x5C42;&#x7684;z</span>
            zs.append(z)

            <span class="hljs-comment"># &#x8BA1;&#x7B97;&#x6BCF;&#x5C42;&#x7684;a</span>
            activation = sigmoid(z)

            <span class="hljs-comment"># &#x4FDD;&#x5B58;&#x6BCF;&#x4E00;&#x5C42;&#x7684;a</span>
            activations.append(activation)

        <span class="hljs-comment"># &#x53CD;&#x5411;&#x66F4;&#x65B0;&#x4E86;</span>
        <span class="hljs-comment"># &#x8BA1;&#x7B97;&#x6700;&#x540E;&#x4E00;&#x5C42;&#x7684;&#x8BEF;&#x5DEE;</span>
        delta = self.cost_derivative(activations[<span class="hljs-number">-1</span>], y) * sigmoid_prime(zs[<span class="hljs-number">-1</span>])

        <span class="hljs-comment"># &#x6700;&#x540E;&#x4E00;&#x5C42;&#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E;&#x7684;&#x5012;&#x6570;</span>
        nabla_b[<span class="hljs-number">-1</span>] = delta
        nabla_w[<span class="hljs-number">-1</span>] = np.dot(delta, activations[<span class="hljs-number">-2</span>].transpose())

        <span class="hljs-comment"># &#x5012;&#x6570;&#x7B2C;&#x4E8C;&#x5C42;&#x4E00;&#x76F4;&#x5230;&#x7B2C;&#x4E00;&#x5C42; &#x6743;&#x91CD;&#x548C;&#x504F;&#x7F6E;&#x7684;&#x5012;&#x6570;</span>
        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>, self.num_layers):
            z = zs[-l]

            sp = sigmoid_prime(z)

            <span class="hljs-comment"># &#x5F53;&#x524D;&#x5C42;&#x7684;&#x8BEF;&#x5DEE;</span>
            delta = np.dot(self.weights[-l+<span class="hljs-number">1</span>].transpose(), delta) * sp

            <span class="hljs-comment"># &#x5F53;&#x524D;&#x5C42;&#x504F;&#x7F6E;&#x548C;&#x6743;&#x91CD;&#x7684;&#x5012;&#x6570;</span>
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l - <span class="hljs-number">1</span>].transpose())

        <span class="hljs-keyword">return</span> (nabla_b, nabla_w)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost_derivative</span><span class="hljs-params">(self, output_activation, y)</span>:</span>
        <span class="hljs-keyword">return</span> (output_activation - y)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + np.exp(-z))


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_prime</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> sigmoid(z) * (<span class="hljs-number">1</span> - sigmoid(z))
</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="2.3 梯度下降法.html" class="navigation navigation-prev " aria-label="Previous page: 2.3 梯度下降法">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="2.5 随机梯度下降法.html" class="navigation navigation-next " aria-label="Next page: 2.5 随机梯度下降法">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"2.4 神经网络的训练","level":"1.3.4","depth":2,"next":{"title":"2.5 随机梯度下降法","level":"1.3.5","depth":2,"path":"2.前置神经网络/2.5 随机梯度下降法.md","ref":"2.前置神经网络/2.5 随机梯度下降法.md","articles":[]},"previous":{"title":"2.3 梯度下降法","level":"1.3.3","depth":2,"path":"2.前置神经网络/2.3 梯度下降法.md","ref":"2.前置神经网络/2.3 梯度下降法.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"2.前置神经网络/2.4 神经网络的训练.md","mtime":"2018-09-20T14:48:55.082Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2018-09-21T06:28:54.634Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

