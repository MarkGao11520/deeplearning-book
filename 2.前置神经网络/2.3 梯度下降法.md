# 2.3 梯度下降法

- #### 线性回归的训练

  线性回归的训练，其实和基于统计的机器学习训练很相近。

  如果你熟悉回归的话，你会觉得bp神经网络的训练很简单。

  线性回归的训练过程: 一个线性回归的模型是怎么学到的，以及它学到了什么。
  
  为理解BP神经网络的训练做铺垫。

- ####  一元线性回归

  一元线性回归是所有机器学习中最简单的。

  样本:

​	![mark](http://myphoto.mtianyan.cn/blog/180329/5BckL08ID3.png?imageslim)

​	观察到很多数据对，有x有y。

​	观察记录(通过画图):

​	![mark](http://myphoto.mtianyan.cn/blog/180329/ahBl9aAjkK.png?imageslim)



​	图中横坐标代表时间，单位是秒。纵坐标代表速度，单位是米每秒。

​	可以从图中直观的看到，时间和速度有一种线性关系。我们可以尝试画一条直线，从一堆点中穿越过去。

​	![mark](http://myphoto.mtianyan.cn/blog/180329/a03LijhmL5.png?imageslim)

​	这条直线几乎可以满足所有点。我们就可以用这条直线来描述图片上的横纵坐标关系。

​	横坐标用x或t表示，纵坐标用y或s表示。

​	横纵坐标的关系: y = wx+b

​	因为直线并不是穿过所有的点，会存在误差。

​	误差:

​	参数e代表error，表示误差

> y =wx+b+e

​	我们取定任何一个w和b时,只要带入x,y就会产生一个e.产生十个e.

​	把表达式变为:

​	![mark](http://myphoto.mtianyan.cn/blog/180329/J0f0171ik8.png?imageslim)

​	这里的下标i就表示的是第几个样本: 也就是y的真实值减去我们的预测值(拟合值y帽子)

​	全局误差总量:

​	![mark](http://myphoto.mtianyan.cn/blog/180329/9mKAkE3983.png?imageslim)

​	将所有单个样本的误差进行累加获得全局误差总量。

> 但是无论这个e是正是负都是误差，这就是残差。这里简单的累加就存在正负抵消。

​	对于每个e取绝对值或取平方保证非负数。

![mark](http://myphoto.mtianyan.cn/blog/180329/L92AfLiiCl.png?imageslim)	

对于第二行表达式进行展开，得到第三行表达式。

> 表达式中的x与y是常数。里面的w和b才是未知数。

​	对于三式，提取公因式并合并就可以得到如下图公式。

 	![mark](http://myphoto.mtianyan.cn/blog/180329/J37dHABa9k.png?imageslim)

> 其中的ABCDEF全部都是常数系数。未知数是w和b

​	接下来我们要做的事情就是找到一个比较合适的w和b，使得loss值越小越好。

​	越接近0，说明我们拟合的误差越小。要让我们的直线，尽可能离样本点近。



​	![mark](http://myphoto.mtianyan.cn/blog/180329/L5em828ah5.png?imageslim)

​	我们找到上面方程ABCDEF的近似方程:



​	![mark](http://myphoto.mtianyan.cn/blog/180329/h52fdkflFe.png?imageslim)

​	这个图形在三维空间中像一个碗。和二维空间中的抛物线很像，肯定存在极值，极值在碗底。

​	只要我们求的碗底的(x,y)我们就求的了最佳参数(w,b)，这里只是符号字母不同，意义是一样的。只是把其中的w,b替换成了x和y。



梯度下降法具体参考：https://www.jianshu.com/p/e5dfe52773e1

​				       https://www.jianshu.com/p/bf50fc0aa9f3