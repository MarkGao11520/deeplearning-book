# 2.6 前馈神经网络手写数字识别

在实现了我们的神经网络之后，我们来看一下如何使用我们刚才的神经网络来做图片的识别。

案例: 手写数字识别

![image-20180920225131448](/Users/gaowenfeng/Library/Application Support/typora-user-images/image-20180920225131448.png)

MNIST dataset

每一张图片为28,28的。数字总共有10种。

### 如何用我们之前实现的神经网络来实现手写数字识别

随机梯度下降，正向传播，反向更新。

```python
if __name__ == '__main__':
    import mnist_loader

    traning_data, validation_data, test_data = mnist_loader.load_data_wrapper()
```

从mnist中取训练数据，验证数据(x为784\*1的向量，y为10\*1的向量)，测试数据(x为784\*1的向量，y为一个实数)。

使用我们的Network类定义我们的网络。定义一个三层的网络,输入层为28,28共784个神经元。隐藏层我们定义30个神经元，输出层有十种数字。    net = Network([784, 30, 10])

```python
    net = Network([784, 30, 10])
```

调用net的SGD方法训练我们的模型:

传入参数： 训练数据集，epoch数量，minibatch大小，学习率0.5

```python
    net.SGD(traning_data, 30, 10, 0.5, test_data=test_data)
```

我们再添加一个我们的参数: test_data。每一轮训练完成之后看我们的神经网络在测试集上的表现如何。

```python
    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
                    if test_data:
            n_test = len(test_data)
```

在训练完成之后进行测试集上表现的测试

```python
            # 测试集上的表现
            if test_data:
                print("Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test))
```

自定义我们的评估evaluate方法,看当前网络总共预测对了多少:

```python
 def evaluate(self, test_data):
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)
```

循环遍历test_data中的数据x和label y。可以让我们取出来的x经过一个函数`feedforward`前向传播就是我们网络的预测结果。因为我们刚才定义的输出层有十个神经元。
 每一维代表它是这个数字的可能性。调用argmax取这十维中最大的一个数字。最大的数字就是我们经过网络预测出来最可能的结果。每一对预测结果和真实标签组成我们的test_results。

如果一样就返回1，不一样就返回0。返回10000个比较结果，0或者1.求和一下就是验证正确的总数。

feedforward就是我们的前向传播函数。输入值a

```python
    def feedforward(self, a):
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a) + b)
        return a
```

遍历当前网络所有层上的w和b。

调用每一个神经元都由线性单元wx+b 经过激励函数组成。网络的预测结果，然后把网络的预测结果返回出去。



![mark](http://myphoto.mtianyan.cn/blog/180331/bF77Lf1CG6.png?imageslim)

解决几个Python3下的报错。

- no module named cPickle

Python3下cpickle改名为了pickle。直接修改修改使用pickle就行了。

- 报错2

```
UnicodeDecodeError: 'ascii' codec can't decode byte 0x90 in position 614: or
```

这个是因为pickle要指定编码格式。

```
    training_data, validation_data, test_data = pickle.load(f,encoding='bytes')
```

添加encoding参数。

- 报错3:

```
TypeError: object of type 'zip' has no len()
```

将zip对象，强制转化为list类型。

```
    training_data = list(zip(training_inputs, training_results))
```

### 训练结果



![mark](http://myphoto.mtianyan.cn/blog/180331/80767EI2Hj.png?imageslim)

隐藏层30 输出层10(代表10种数字0-9) 学习率0.5

跑完30轮之后准确率还蛮高的93%。

![mark](http://myphoto.mtianyan.cn/blog/180331/a2g9F9jBkm.png?imageslim)

只改变了隐藏层的神经元个数，准确率提高到了94%。



#### 总结

实际上，我们每一个样本是多少维（m）的，输入数据就是有多少个神经元，最终的分类有多少类（n），就有多少个输出神经元，中间的隐藏层的神经元（x1,x2,x3）是我们调整的。

所以其实前馈神经网络，每一层就是在做一个矩阵操作` sigmiod([m*1](输入神经元) ·[m*x1](每一层权重)+[m*1](每一层偏置\))`  这很像是逻辑回归，不过逻辑回归解决的是二分类问题，而这个把结果分类映射成了一个向量，从而可以解决多分类问题。但是输入数据的每一维数据，最终映射到输出数据的每一维的过程，就很像是一个二分类的逻辑回归了