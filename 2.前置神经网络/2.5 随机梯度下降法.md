# 2.5 随机梯度下降法

在介绍梯度下降算法时我们已经介绍了损失函数的公式了。在损失函数的公式里面n表示拥有n个样本。表示求导数的过程中n个样本都要参与计算。

![mark](http://myphoto.mtianyan.cn/blog/180331/BKF53mE03m.png?imageslim)

可是n的值往往非常大，每个样本都有x个维度，有可能数万，有可能数百万。这种计算需要大量的时间。需要有一种方式缩短这种计算时间。

![mark](http://myphoto.mtianyan.cn/blog/180331/aDcgDkGm3E.png?imageslim)

随机梯度下降算法其实没有什么新奇的地方，就是统计学中的抽样。我们从整体的数据中随机的抽取一部分样本，这些样本的特征已经一定程度上代表样本集的特征。

就像统计人类的特征，不需要世界人口普查，只需要抽取一部分具有代表性的特征。只要他们能覆盖所有的人口，年龄，性别。

对这一部分抽取出来的样本进行归纳总结就可以了。

SGD中的几个名词

- 减少计算量

mini-batch: 从训练样本中随机选取一些数据

epoch: 一次训练一个mini-batch,直到把所有训练数据都用一遍。

这种情况下w和b的更新公式也需要做相应的修改。只要对后面一项除以m就可以了。

![mark](http://myphoto.mtianyan.cn/blog/180331/7hJEdECjFi.png?imageslim)

m的大小就是mini-batch的大小。从训练样本中随机抽取的数据的大小就是m的大小。

## SGD代码更新

在了解了随机梯度下降算法之后我们又可以更新之前的算法。

第19行，把梯度下降函数改为SGD随机梯度下降。函数中再多添加一个参数叫做mini_batch_size

```python
    # 随机梯度下降
    def SGD(self, training_data, epochs, mini_batch_size, eta):
        # 取出训练数据总个数
        n = len(training_data)
```

在洗牌之后，我们循环mini_batch_size

```python
# mini_batch
            mini_batches = [training_data[k:k + mini_batch_size]
                            for k in range(0, n, mini_batch_size)]
```

我们循环0-n,每隔mini_batch_size取一个k。training_data从k开始一直到k加上minibatch

之前是训练整个数据集，我们现在只需要训练minibatch大小个数据就可以了

```python
 # 训练mini_batch
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
```

我们需要实现一个新的函数。update_mini_batch

```python
# 更新mini_batch
    def update_mini_batch(self, mini_batch, eta):
        # 保存每层偏倒
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]

        # 训练每一个mini_batch
        for x, y in mini_batch:
            delta_nable_b, delta_nabla_w = self.update(x, y)

            # 保存一次训练网络中每层的偏倒
            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nable_b)]
            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

        # 更新权重和偏置 Wn+1 = wn - eta * nw
        self.weights = [w - (eta / len(mini_batch)) * nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b - (eta / len(mini_batch)) * nb
                       for b, nb in zip(self.biases, nabla_b)]
```

将training_data改为mini_batch

更新权重和偏置的时候使用eta除以minibatch的总个数。